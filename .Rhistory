<<<<<<< HEAD
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
## replace '/', used sometimes to separate alternatives w a space
## replace '@' in email addresses
## replace '\\' from URLS
for (j in seq(myCorpus))
{
myCorpus[[j]] <- gsub("/", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("@", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("\\|", " ", myCorpus[[j]])
}
## add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
## task-specific transformations (pw to pathway, etc)
# for (j in seq(myCorpus))
# {
#     myCorpus[[j]] <- gsub("harbin institute technology", "HIT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("shenzhen institutes advanced technology", "SIAT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("chinese academy sciences", "CAS", myCorpus[[j]])
# }
## let's convert to a plain text document before going further. We
## need this step to create the document term matrix
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
## create the term matrix
dtm = DocumentTermMatrix(myCorpus)
## get term frequencies
freq = colSums(as.matrix(dtm))
m = as.matrix(dtm)
View(m)
View(df)
head(df)
head(myCorpus)
inspect(myCorpus[[100:105]])
inspect(myCorpus[100:105])
rdmTweets = searchTwitter("ebola", n = 400, cainfo="cacert.pem", language="english")
rdmTweets = searchTwitter("ebola", n = 400, cainfo="cacert.pem", language="en")
rdmTweets = searchTwitter("ebola", n = 400, cainfo="cacert.pem", lang="en")
nDocs = length(rdmTweets)
## convert the tweets to a data frame
df = do.call("rbind", lapply(rdmTweets, as.data.frame))
## build a corpus
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
## TODO: DECIDE IF YOU NEED TO REMOVE NUMBERS, OR IF YOU NEED THEM
## SKIP THIS STEP!!!
myCorpus <- tm_map(myCorpus, removeNumbers)
## inspecting the corpus
inspect(myCorpus[4:10])
## remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
## replace '/', used sometimes to separate alternatives w a space
## replace '@' in email addresses
## replace '\\' from URLS
for (j in seq(myCorpus))
{
myCorpus[[j]] <- gsub("/", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("@", " ", myCorpus[[j]])
myCorpus[[j]] <- gsub("\\|", " ", myCorpus[[j]])
}
## add two extra stop words: "available" and "via"
myStopwords <- c(stopwords('english'), "available", "via")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
## task-specific transformations (pw to pathway, etc)
# for (j in seq(myCorpus))
# {
#     myCorpus[[j]] <- gsub("harbin institute technology", "HIT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("shenzhen institutes advanced technology", "SIAT", myCorpus[[j]])
#     myCorpus[[j]] <- gsub("chinese academy sciences", "CAS", myCorpus[[j]])
# }
## let's convert to a plain text document before going further. We
## need this step to create the document term matrix
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
## create the term matrix
dtm = DocumentTermMatrix(myCorpus)
## get term frequencies
freq = colSums(as.matrix(dtm))
m = as.matrix(dtm)
View(df)
View(m)
dim(m)
write.csv(m, "C://data//wordmatrix.csv")
dtm2 = removeSparseTerms(dtm, 0.1)
dim(dtm)
dim(dtm2)
names(dtm2)
dtm3 = removeSparseTerms(dtm, 0.9)
dim(dtm3)
names(dtm3)
dtm3$Terms
dtm3 = removeSparseTerms(dtm, 0.99)
dim(dtm3)
freq = colSums(as.matrix(dtm3))
freq
freq = colSums(as.matrix(dtm2))
freq
dtm3 = removeSparseTerms(dtm, 0.9)
freq = colSums(as.matrix(dtm3))
freq
findFreqTerms(dtm, lowfreq = 200)
findFreqTerms(dtm, lowfreq = 20)
findAssocs(dtm, "treat", corlimit = 0.3)
plot(dtm, terms=findFreqTerms(dtm, lowfreq=40)[1:50], corThreshold=0.5)
source("http://bioconductor.org/biocLite.R")
biocLite("Rgraphviz")
plot(dtm, terms=findFreqTerms(dtm, lowfreq=40)[1:50], corThreshold=0.5)
plot(dtm, terms=findFreqTerms(dtm, lowfreq=40)[1:50], corThreshold=0.5)
plot(dtm, terms=findFreqTerms(dtm, lowfreq=20)[1:50], corThreshold=0.5)
plot(dtm, terms=findFreqTerms(dtm, lowfreq=20)[1:10], corThreshold=0.5)
library(wordcloud)
install.packages("wordcloud")
library(wordcloud)
wordcloud(names(freq), freq, min.freq(10))
wordcloud(names(freq), freq, min.freq=10)
wordcloud(names(freq), freq, min.freq=5)
wordcloud(names(freq), freq, min.freq=3)
wordcloud(names(freq), freq, min.freq=2)
wordcloud(names(freq), freq, min.freq=1)
table(freq)
freq
freq = colSums(as.matrix(dtm))
wordcloud(names(freq), freq, min.freq=1)
wordcloud(names(freq), freq, min.freq=10)
wordcloud(names(freq), freq, min.freq=50)
wordcloud(names(freq), freq, min.freq=20)
wordcloud(names(freq), freq, min.freq=10)
wordcloud(names(freq), freq, min.freq=10, colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 0.1), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 0.2), colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, scale=c(5, 0.2), colors=brewer.pal(6, "Dark2"))
rm(list = ls())
load("twitteR_credentials")
registerTwitterOAuth(twitCred)
rdmTweets = searchTwitter("saints", n = 400, cainfo="cacert.pem", lang="en")
nDocs = length(rdmTweets)
## convert the tweets to a data frame
df = do.call("rbind", lapply(rdmTweets, as.data.frame))
## build a corpus
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
inspect(myCorpus[4:10])
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
## replace '/', used sometimes to separate alternatives w a space
myStopwords <- c(stopwords('english'), "available", "via")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
inspect(myCorpus[4:10])
myCorpusCopy = myCorpus
## let's convert to a plain text document before going further. We
## need this step to create the document term matrix
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
inspect(myCorpus[4:10])
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
myCorpus = tm_map(myCorpus, PlainTextDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
myCorpusCopy = tm_map(myCorpusCopy, PlainTextDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
inspect(myCorpus[41:50])
dtm = DocumentTermMatrix(myCorpus)
dtm = DocumentTermMatrix(myCorpus)
myCorpus = tm_map(myCorpus, PlainTextDocument)
dtm = DocumentTermMatrix(myCorpus)
findFreqTerms(dtm, lowfreq=100)
findFreqTerms(dtm, lowfreq=50)
findFreqTerms(dtm, lowfreq=10)
findAssocs(dtm, "data", corlimit=0.6)
findAssocs(dtm, "brees", corlimit=0.6)
inspect(myCorpus[41:50])
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
inspect(myCorpus[41:50])
## remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords('english'), "available", "via")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
inspect(myCorpus[41:50])
dtm = DocumentTermMatrix(myCorpus)
# Explore the corpus.
findFreqTerms(dtm, lowfreq=10)
findAssocs(dtm, "brees", corlimit=0.6)
findAssocs(dtm, "atlanta", corlimit=0.6)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
library(ggplot2)
p <- ggplot(subset(wf, freq>50), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
# Generate a word cloud
library(wordcloud)
wordcloud(names(freq), freq, min.freq=100, colors=brewer.pal(6, "Dark2")
wordcloud(names(freq), freq, min.freq=100, colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=100, colors=brewer.pal(6, "Dark2"))
wordcloud(names(freq), freq, min.freq=10, colors=brewer.pal(6, "Dark2"))
p <- ggplot(subset(wf, freq>50), aes(word, freq))
load("twitteR_credentials")
registerTwitterOAuth(twitCred)
## searching someone's timeline
## rdmTweets = userTimeline("rdatamining", n = 400, cainfo="cacert.pem")
## searching for a term
rdmTweets = searchTwitter("Obama", n = 400, cainfo="cacert.pem", lang="en")
nDocs = length(rdmTweets)
## convert the tweets to a data frame
df = do.call("rbind", lapply(rdmTweets, as.data.frame))
## build a corpus
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
inspect(myCorpus[41:50])
myCorpus <- tm_map(myCorpus, removeNumbers)
## inspecting the corpus
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
## replace '/', used sometimes to separate alternatives w a space
## replace '@' in email addresses
## replace '\\' from URLS
myStopwords <- c(stopwords('english'), "available", "via", "obama")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
inspect(myCorpus[41:50])
myCorpusCopy = myCorpus
myCorpusCopy = tm_map(myCorpusCopy, PlainTextDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy)
warnings()
inspect(myCorpus[41:50])
myCorpus <- tm_map(myCorpus, stripWhitespace)
inspect(myCorpus[41:50])
dtm = DocumentTermMatrix(myCorpus)
myCorpus = tm_map(myCorpus, PlainTextDocument)
dtm = DocumentTermMatrix(myCorpus)
findFreqTerms(dtm, lowfreq=10)
findAssocs(dtm, "obama", corlimit=0.6)
findAssocs(dtm, "potus", corlimit=0.6)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
inspect(myCorpus[41:50])
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
## TODO: DECIDE IF YOU NEED TO REMOVE NUMBERS, OR IF YOU NEED THEM
## SKIP THIS STEP!!!
myCorpus <- tm_map(myCorpus, removeNumbers)
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords('english'), "available", "via", "obama")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
dtm = DocumentTermMatrix(myCorpus)
# Explore the corpus.
findFreqTerms(dtm, lowfreq=10)
myCorpusCopy = myCorpus
myCorpusCopy = tm_map(myCorpusCopy, PlainTextDocument)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy, type = prevalent)
myCorpus <- tm_map(myCorpus, stemCompletion, dictionary=myCorpusCopy, type = "prevalent")
warnings()
myCorpus = Corpus(VectorSource(df$text))
## cleanup
## make everything lowercase
myCorpus = tm_map(myCorpus, tolower)
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
## remove numbers
## TODO: DECIDE IF YOU NEED TO REMOVE NUMBERS, OR IF YOU NEED THEM
## SKIP THIS STEP!!!
myCorpus <- tm_map(myCorpus, removeNumbers)
## inspecting the corpus
inspect(myCorpus[41:50])
## remove URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
myCorpus <- tm_map(myCorpus, removeURL)
myStopwords <- c(stopwords('english'), "available", "via", "obama")
## remove "r" and "big" from stopwords
myStopwords <- setdiff(myStopwords, c("r", "big"))
## remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
## strip whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
myCorpus = tm_map(myCorpus, PlainTextDocument)
#stem document
myCorpus = tm_map(myCorpus, stemDocument)
dtm = DocumentTermMatrix(myCorpus)
# Explore the corpus.
findFreqTerms(dtm, lowfreq=10)
findAssocs(dtm, "potus", corlimit=0.6)
findAssocs(dtm, "lesbian", corlimit=0.6)
freq <- sort(colSums(as.matrix(dtm)), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
library(ggplot2)
p <- ggplot(subset(wf, freq>50), aes(word, freq))
p <- p + geom_bar(stat="identity")
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))
# Generate a word cloud
library(wordcloud)
wordcloud(names(freq), freq, min.freq=10, colors=brewer.pal(6, "Dark2"))
myTdm <- TermDocumentMatrix(myCorpus, control=list(wordLengths=c(1,Inf)))
myTdm
idx <- which(dimnames(myTdm)$Terms == "r")
inspect(myTdm[idx+(0:5),101:110])
findFreqTerms(myTdm, lowfreq=10)
termFrequency <- rowSums(as.matrix(myTdm))
termFrequency <- subset(termFrequency, termFrequency>=10)
qplot(names(termFrequency), termFrequency, geom="bar", xlab="Terms") + coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms") + coord_flip()
termFrequency
qplot(names(termFrequency), geom="bar", xlab="Terms", ylim = 35) + coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms", xlim = 35) + coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms", ylim = 35, xlim=length(termFrequency)) + coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms") #+ coord_flip()
qplot(names(termFrequency), geom="bar", xlab="Terms") + coord_flip()
library(ggplot2)
qplot(names(termFrequency), geom="bar", xlab="Terms") + coord_flip()
myTdm2 = removeSparseTerms(myTdm, sparse=0.95)
dim(myTdm2)
m2 = as.matrix(myTdm2)
distMatrix = dist(scale(m2))
fit = hclust(distMatrix, method="ward")
fit = hclust(distMatrix, method="ward.D2")
plot(fit)
rect.hclust(fit, k=10)
acs = read.csv("c:\\data\\acs.csv")
rm(list = ls())
acs = read.csv("c:\\data\\acs.csv")
names(acs)
sset = acs[which(acs$VAL == 24), ]
download.file(url = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx", destfile = "c:\\data\\ngap.xlsx", mode = "wb")
rows = 18:23
cols = 7:15
dat = read.xlsx("c:\\data\\ngap.xlsx", sheetIndex = 1, colIndex = cols, rowIndex = rows)
library(xlsx)
dat = read.xlsx("c:\\data\\ngap.xlsx", sheetIndex = 1, colIndex = cols, rowIndex = rows)
sum(dat$Zip*dat$Ext,na.rm=T)
download.file(url = "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", destfile = "c:\\data\\restaurants.xml")
DT = fread("c:\\data\\restaurants.xml", sep="auto", sep2="auto", nrows=-1L, header="auto", na.strings="NA",
stringsAsFactors=FALSE, verbose=FALSE, autostart=30L, skip=-1L, select=NULL,
colClasses=NULL, integer64=getOption("datatable.integer64"))
library(data.table)
rm(list = ls())
rm(df)
rm(list = ls())
mem()
ls()
data()
str(Nile)
ToothGrowth?
?ToothGrowth
str(ToothGrowth)
ToothGrowth
qplot(dose, len, data=ToothGrowth)
library(ggplot2)
qplot(dose, len, data=ToothGrowth)
qplot(dose, len, data=ToothGrowth, color = supp)
qplot(dose, len, data=ToothGrowth, color = supp, geom = c("point", "smooth"), method = "lm")
install.packages("reshape2")
install.packages("reshape2")
install.packages("reshape2")
library(plyr)
library(ggplot2)
library(reshape2)
install.packages(c("digest", "httr", "manipulate", "mvtnorm", "quantmod", "rattle", "RColorBrewer", "RCurl", "rmarkdown"))
install.packages(c("digest", "httr", "manipulate", "mvtnorm",
install.packages("reshape2")
library(data.table)
library(plyr)
library(stringr)
library(ggplot2)
library(maps)
install.packages("maps")
install.packages("bit64")
install.packages("RColorBrewer")
install.packages("choroplethr")
library(maps)
library(bit64)
library(RColorBrewer)
library(choroplethr)
getwd()
source('~/GitHub/HomeCode/cookbook.R')
library(dplyr)
ann2012 = fread("c:\\data\\2012.annual.singlefile.csv")
source('~/GitHub/HomeCode/cookbook.R')
source('~/GitHub/homecode/test.R')
rm(list = ls())
library(data.table)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(maps)
library(bit64)
library(RColorBrewer)
library(choroplethr)
dt = fread("c:\\data\\2012.annual.singlefile.csv", sep=",", colClasses=c("character", rep("integer", 5),
"character", "character",
"integer64", "integer64",
"integer64", "integer64", "integer64",
"integer", "integer", "character",
rep("integer", 7)))
## take a peek at the data
dim(dt)
head(dt)
View(dt)
library(data.table)
library(plyr)
library(dplyr)
library(stringr)
library(ggplot2)
library(maps)
library(bit64)
library(RColorBrewer)
library(choroplethr)
simpleCap = function(x) {
if(!is.na(x)) {
s = strsplit(x, " ")[[1]]
paste(toupper(substring(s, 1, 1)), substring(s, 2), sep = "", collapse = " ")
} else (NA)
}
## read in the employment data
## additional data
for (u in c("agglevel", "area", "industry", "ownership", "size")) {
assign(u, read.csv(paste("c:\\data\\", u, "_titles.csv", sep=""), stringsAsFactors=F))
}
codes = c("agglevel", "area", "industry", "ownership", "size")
dtfull = dt
## clean up factors
dtfull$agglvl_code = as.integer(dtfull$agglvl_code)
dtfull2 = merge(dtfull, agglevel, by="agglvl_code", all.x = TRUE)
dtfull2 = merge(dtfull2, area, by="area_fips", all.x = TRUE)
dtfull2 = merge(dtfull2, industry, by="industry_code", all.x = TRUE)
dtfull2 = merge(dtfull2, ownership2, by="own_code", all.x = TRUE)
dtfull2 = merge(dtfull2, size2, by="size_code", all.x = TRUE)
data(county.fips)
county.fips$fips = str_pad(county.fips$fips, width = 5, pad = "0")
county.fips$polyname = as.character(county.fips$polyname)
county.fips$county = sapply(gsub('[a-z\ ]+, ([a-z\ ]+)','\\1', county.fips$polyname),simpleCap)
county.fips = unique(county.fips)
data(state.fips)
state.fips$fips = str_pad(state.fips$fips, width=2, pad="0", side = 'left')
state.fips$state = as.character(state.fips$polyname)
state.fips$state = gsub("([a-z\ ]+):[a-z\ \\']+", '\\1', state.fips$state)
state.fips$state = sapply(state.fips$state, simpleCap)
mystatefips = unique(state.fips[,c('fips', 'abb', 'state')])
dtfull2$own_code = as.integer(dtfull2$own_code)
dtfull2 = merge(dtfull2, ownership2, by="own_code", all.x = TRUE)
dtfull2 = merge(dtfull2, ownership, by="own_code", all.x = TRUE)
dtfull2 = merge(dtfull2, size, by="size_code", all.x = TRUE)
dtfull2$size_code = as.integer(dtfull2$size_code)
dtfull2 = merge(dtfull2, size, by="size_code", all.x = TRUE)
=======
library(lattice)
library(ggplot2)
install.packages("ggplot2")
library(plyr)
## set working directory
setwd(".\\Github\\EDA-Project2")
## assumes that the source data files are in a subfolder named 'data'
## NEI will contain all of the PM2.5 emissions data for 1999, 2002, 2005, and 2008.
## For each year, the table contains number of tons of PM2.5 emitted from a specific
## type of source for the entire year.
NEI <- readRDS("data\\summarySCC_PM25.rds")
SCC <- readRDS("data\\Source_Classification_Code.rds")
## Plot 1
## Have total emissions from PM2.5 decreased in the United States from 1999 to 2008?
## Using the base plotting system, make a plot showing the total PM2.5 emission from
## all sources for each of the years 1999, 2002, 2005, and 2008.
## Get aggregate totals for each year
totalEmissions = aggregate(Emissions ~ year, data = NEI, sum)
barplot(totalEmissions$Emissions,
names.arg = totalEmissions$year,
main = "US Total Emissions",
xlab = "Year",
ylab = expression('PM'[25]*' Emissions (Tons)') )
balt = NEI[which(NEI$fips == "24510"), ]
## convert the 'year' variable to a factor
balt$year = as.factor(balt$year)
## aggregate the Baltimore results
baltEmissions = aggregate(Emissions ~ year, data = balt, sum)
barplot(baltEmissions$Emissions,
names.arg = baltEmissions$year,
main = "Baltimore Total Emissions",
xlab = "Year",
ylab = expression('PM'[25]*' Emissions (Tons)') )
balt = NEI[which(NEI$fips == "24510"), ]
## convert the 'year' variable to a factor
balt$year = as.factor(balt$year)
## get the summary data
baltEmissions = ddply(balt, c("year", "type"), summarize, sum = sum(Emissions))
qplot(data = baltEmissions,
x = year,
y = sum,
ylab = expression('PM'[25]*' Emissions (Tons)'),
xlab = "Year",
main = "Baltimore Emission Sources",
fill = type,
geom = "bar",
stat="identity",
position = "dodge")
install.packages("manipulate")
qplot(data = baltEmissions,
x = year,
y = sum,
ylab = expression('PM'[25]*' Emissions (Tons)'),
xlab = "Year",
main = "Baltimore Emission Sources",
fill = type,
geom = "bar",
stat="identity",
position = "dodge")
library(lattice)
library(ggplot2)
library(plyr)
tEmissions = ddply(balt, c("year", "type"), summarize, sum = sum(Emissions))
qplot(data = baltEmissions,
x = year,
y = sum,
ylab = expression('PM'[25]*' Emissions (Tons)'),
xlab = "Year",
main = "Baltimore Emission Sources",
fill = type,
geom = "bar",
stat="identity",
position = "dodge")
vehicle_scc = SCC[grep("Vehicle", SCC$EI.Sector) , ]
vehicle = NEI[NEI$SCC %in% vehicle_scc$SCC, ]
## get the data for the target cities
cities = vehicle[vehicle$fips %in% c("24510", "06037"), ]
cities$fips[which(cities$fips == "06037")] = "Los Angeles"
cities$fips[which(cities$fips == "24510")] = "Baltimore"
names(cities)[1] = "Cities"
cityEmissions = ddply(cities, c("year", "Cities"), summarize, sum = sum(Emissions))
cityEmissions$year = as.factor(cityEmissions$year)
cityEmissions$year = as.factor(cityEmissions$year)
qplot(data = cityEmissions,
x = year,
y = sum,
ylab = expression('PM'[25]*' Emissions (Tons)'),
xlab = "Year",
main = "Vehicle Emission Sources",
fill = Cities,
geom = "bar",
stat="identity",
position = "dodge")
library(plyr)
library(ggplot2)
library(reshape2)
setwd("~/GitHub/HomeCode")
veh = read.csv("data\\vehicles.csv", stringsAsFactors = FALSE, sep = ",", na.strings = "")
dim(veh)
head(veh)
unique(veh$trany)
names(veh)
length(unique(veh$year))
first_year = min(veh$year)
last_year = max(veh$year)
barplot(table(veh$fuelType1))
veh$trany2 = ifelse(substr(veh$trany, 1, 4) == "Auto", "Auto", "Manual")
>>>>>>> origin/master
